{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUY759mHEnX2"
   },
   "source": [
    "## Module 3 ( generating Image embedding )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhVrEyDPVfMl",
    "outputId": "95449c47-4648-4861-af74-dbf7cd7b6ce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-08 07:15:32--  https://storage.googleapis.com/kaggle-data-sets/930393/1613771/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210408%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210408T071356Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=543bf16d4aacd031137a27ce91ab974dfc2883d1180c78f2a0a7c7eb3a912101c9445b3d93ca648e33533d76a1a0e6b27c03c6ad0898605125f0616fd4d3aaff7219951948e3b319af1f87436130bc6a2bbcdf25c2337c7960bf33ef7dea86a0175e9eac39d59e76d9c4281b0a0e8f569d942487493762bb239bd6da3eb1322a481bcff60e0b9efeffdce022fcc9dbf9cd7d1c01bb8041c27fbdcae2a6a6c891159156bcc3531a18c2076a0ae790682c3c95345d9b6dae742c78ba0515d6f4525a2e7284917779b993e8a8a2619cfbe17516caa69c9abbdc3e739f526a63359f7aac0e382f6c86171057c2d82c8c83b0847b75ad2d1e20f4b19a673a31b42966\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.164.176, 172.217.164.144, 172.217.12.240, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.164.176|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11894512126 (11G) [application/zip]\n",
      "Saving to: ‘archive.zip’\n",
      "\n",
      "archive.zip         100%[===================>]  11.08G  82.1MB/s    in 4m 24s  \n",
      "\n",
      "2021-04-08 07:19:57 (42.9 MB/s) - ‘archive.zip’ saved [11894512126/11894512126]\n",
      "\n",
      "time: 4min 25s (started: 2021-04-08 07:15:32 +00:00)\n"
     ]
    }
   ],
   "source": [
    "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-IN,en-GB;q=0.9,en-US;q=0.8,en;q=0.7\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-data-sets/930393/1613771/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210408%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210408T071356Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=543bf16d4aacd031137a27ce91ab974dfc2883d1180c78f2a0a7c7eb3a912101c9445b3d93ca648e33533d76a1a0e6b27c03c6ad0898605125f0616fd4d3aaff7219951948e3b319af1f87436130bc6a2bbcdf25c2337c7960bf33ef7dea86a0175e9eac39d59e76d9c4281b0a0e8f569d942487493762bb239bd6da3eb1322a481bcff60e0b9efeffdce022fcc9dbf9cd7d1c01bb8041c27fbdcae2a6a6c891159156bcc3531a18c2076a0ae790682c3c95345d9b6dae742c78ba0515d6f4525a2e7284917779b993e8a8a2619cfbe17516caa69c9abbdc3e739f526a63359f7aac0e382f6c86171057c2d82c8c83b0847b75ad2d1e20f4b19a673a31b42966\" -c -O 'archive.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDF-Fqghemn2"
   },
   "source": [
    "Applications link :  https://www.tensorflow.org/api_docs/python/tf/keras/applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LpbWbxxaWqgY",
    "outputId": "eba85b6b-356d-4ea1-9f83-b6fa1d74c1c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5min 39s (started: 2021-04-08 07:21:21 +00:00)\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!unzip -q '/content/archive.zip' -d '/content/data'\n",
    "!rm -rf '/content/archive.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-FWvDL3ZXA7",
    "outputId": "74b2ddf8-910f-4ff6-9472-6692684fba81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.74 s (started: 2021-04-08 07:32:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KukasHFdho9"
   },
   "source": [
    "## DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oqmAvoSHY-Sd",
    "outputId": "36dacd01-c7a0-47e7-f681-3fea482025ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8min 25s (started: 2021-04-08 07:59:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model_embedding = tf.keras.applications.DenseNet121(include_top=False, weights='imagenet', input_tensor=None, input_shape=(520,520,3),pooling=None,)\n",
    "def load_img(path):\n",
    "  # Reading an image\n",
    "  image = cv2.imread(path)\n",
    "  # resizing because pre-trained model image shape is 520x520\n",
    "  image = cv2.resize(image,(520,520),interpolation=cv2.INTER_AREA)\n",
    "  # Converting to RBG because it will be saved as a correct image even if it is saved after being converted to a PIL\n",
    "  image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "  # Preprocessed numpy.array or a tf.Tensor with type float32.\n",
    "  image = tf.image.convert_image_dtype(image,tf.float32)[tf.newaxis, ...]\n",
    "  return image\n",
    "\n",
    "def get_embeddings(path):\n",
    "  image = load_img(path)\n",
    "  # Retuns 1024 dimension array/ vector with predicted values \n",
    "  img_embedding = model_embedding.predict(image,steps=1)\n",
    "  # Removes dimensions of size 1 from the shape of a tensor.\n",
    "  img_embedding = tf.squeeze(img_embedding, axis=None, name=None)\n",
    "  # Computes the mean of elements across dimensions of a tensor. [ Normalize ]\n",
    "  img_embedding = tf.reduce_mean(img_embedding, axis=(0,1), keepdims=False, name=None).numpy()\n",
    "  # Converting to List\n",
    "  img_embedding = img_embedding.tolist()\n",
    "  return img_embedding\n",
    "\n",
    "DenseNet121_embeddings = []\n",
    "dir = r'/content/data/women_boots'\n",
    "for filename in os. listdir(dir):\n",
    "  if filename.endswith(\".jpg\"):\n",
    "    DenseNet121_embeddings.append(get_embeddings(os.path.join(dir,filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_14udtJyevY3"
   },
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9cjTg2zertd",
    "outputId": "260416f4-bf21-4ab0-e2cb-03f37ff33225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8min 52s (started: 2021-04-08 08:08:09 +00:00)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model_embedding = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_tensor=None,input_shape=(520,520,3), pooling=None,)\n",
    "def load_img(path):\n",
    "  # Reading an image\n",
    "  image = cv2.imread(path)\n",
    "  # resizing because pre-trained model image shape is 520x520\n",
    "  image = cv2.resize(image,(520,520),interpolation=cv2.INTER_AREA)\n",
    "  # Converting to RBG because it will be saved as a correct image even if it is saved after being converted to a PIL\n",
    "  image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "  # Preprocessed numpy.array or a tf.Tensor with type float32.\n",
    "  image = tf.image.convert_image_dtype(image,tf.float32)[tf.newaxis, ...]\n",
    "  return image\n",
    "\n",
    "def get_embeddings(path):\n",
    "  image = load_img(path)\n",
    "  # Retuns 1024 dimension array/ vector with predicted values \n",
    "  img_embedding = model_embedding.predict(image,steps=1)\n",
    "  # Removes dimensions of size 1 from the shape of a tensor.\n",
    "  img_embedding = tf.squeeze(img_embedding, axis=None, name=None)\n",
    "  # Computes the mean of elements across dimensions of a tensor. [ Normalize ]\n",
    "  img_embedding = tf.reduce_mean(img_embedding, axis=(0,1), keepdims=False, name=None).numpy()\n",
    "  # Converting to List\n",
    "  img_embedding = img_embedding.tolist()\n",
    "  return img_embedding\n",
    "\n",
    "ResNet50_embeddings = []\n",
    "dir = r'/content/data/women_boots'\n",
    "for filename in os. listdir(dir):\n",
    "  if filename.endswith(\".jpg\"):\n",
    "    ResNet50_embeddings.append(get_embeddings(os.path.join(dir,filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Sxx8AwLgBWd"
   },
   "source": [
    "## ResNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cWl4F2-xgDyT",
    "outputId": "f6120127-006b-4570-95aa-668f1c08d6da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "171450368/171446536 [==============================] - 2s 0us/step\n",
      "time: 9min 47s (started: 2021-04-08 08:17:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model_embedding = tf.keras.applications.ResNet101(include_top=False, weights='imagenet', input_tensor=None,input_shape=(520,520,3), pooling=None,)\n",
    "def load_img(path):\n",
    "  # Reading an image\n",
    "  image = cv2.imread(path)\n",
    "  # resizing because pre-trained model image shape is 520x520\n",
    "  image = cv2.resize(image,(520,520),interpolation=cv2.INTER_AREA)\n",
    "  # Converting to RBG because it will be saved as a correct image even if it is saved after being converted to a PIL\n",
    "  image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "  # Preprocessed numpy.array or a tf.Tensor with type float32.\n",
    "  image = tf.image.convert_image_dtype(image,tf.float32)[tf.newaxis, ...]\n",
    "  return image\n",
    "\n",
    "def get_embeddings(path):\n",
    "  image = load_img(path)\n",
    "  # Retuns 1024 dimension array/ vector with predicted values \n",
    "  img_embedding = model_embedding.predict(image,steps=1)\n",
    "  # Removes dimensions of size 1 from the shape of a tensor.\n",
    "  img_embedding = tf.squeeze(img_embedding, axis=None, name=None)\n",
    "  # Computes the mean of elements across dimensions of a tensor. [ Normalize ]\n",
    "  img_embedding = tf.reduce_mean(img_embedding, axis=(0,1), keepdims=False, name=None).numpy()\n",
    "  # Converting to List\n",
    "  img_embedding = img_embedding.tolist()\n",
    "  return img_embedding\n",
    "\n",
    "ResNet101_embeddings = []\n",
    "dir = r'/content/data/women_boots'\n",
    "for filename in os. listdir(dir):\n",
    "  if filename.endswith(\".jpg\"):\n",
    "    ResNet101_embeddings.append(get_embeddings(os.path.join(dir,filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMFT4Z1Igk--"
   },
   "source": [
    "## MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QvvaDvigkN8",
    "outputId": "a810eeee-052f-4755-8f4a-873d48f5cde8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 0s 0us/step\n",
      "time: 6min 27s (started: 2021-04-08 08:37:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model_embedding = tf.keras.applications.MobileNet(input_shape=(520,520,3), alpha=1.0, \n",
    "                                                  depth_multiplier=1, dropout=0.001,include_top=False, weights='imagenet', input_tensor=None, pooling=None,)\n",
    "def load_img(path):\n",
    "  # Reading an image\n",
    "  image = cv2.imread(path)\n",
    "  # resizing because pre-trained model image shape is 520x520\n",
    "  image = cv2.resize(image,(520,520),interpolation=cv2.INTER_AREA)\n",
    "  # Converting to RBG because it will be saved as a correct image even if it is saved after being converted to a PIL\n",
    "  image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "  # Preprocessed numpy.array or a tf.Tensor with type float32.\n",
    "  image = tf.image.convert_image_dtype(image,tf.float32)[tf.newaxis, ...]\n",
    "  return image\n",
    "\n",
    "def get_embeddings(path):\n",
    "  image = load_img(path)\n",
    "  # Retuns 1024 dimension array/ vector with predicted values \n",
    "  img_embedding = model_embedding.predict(image,steps=1)\n",
    "  # Removes dimensions of size 1 from the shape of a tensor.\n",
    "  img_embedding = tf.squeeze(img_embedding, axis=None, name=None)\n",
    "  # Computes the mean of elements across dimensions of a tensor. [ Normalize ]\n",
    "  img_embedding = tf.reduce_mean(img_embedding, axis=(0,1), keepdims=False, name=None).numpy()\n",
    "  # Converting to List\n",
    "  img_embedding = img_embedding.tolist()\n",
    "  return img_embedding\n",
    "\n",
    "MobileNet_embeddings = []\n",
    "dir = r'/content/data/women_boots'\n",
    "for filename in os. listdir(dir):\n",
    "  if filename.endswith(\".jpg\"):\n",
    "    MobileNet_embeddings.append(get_embeddings(os.path.join(dir,filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1es39bXB3u_"
   },
   "source": [
    "##  InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wbre18IZk9Sy",
    "outputId": "8dc9b640-ba09-4fb7-e541-56a5f32f324b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 0s 0us/step\n",
      "time: 8min 59s (started: 2021-04-08 08:43:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model_embedding = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet', input_tensor=None,input_shape=(520,520,3), pooling=None,)\n",
    "def load_img(path):\n",
    "  # Reading an image\n",
    "  image = cv2.imread(path)\n",
    "  # resizing because pre-trained model image shape is 520x520\n",
    "  image = cv2.resize(image,(520,520),interpolation=cv2.INTER_AREA)\n",
    "  # Converting to RBG because it will be saved as a correct image even if it is saved after being converted to a PIL\n",
    "  image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "  # Preprocessed numpy.array or a tf.Tensor with type float32.\n",
    "  image = tf.image.convert_image_dtype(image,tf.float32)[tf.newaxis, ...]\n",
    "  return image\n",
    "\n",
    "def get_embeddings(path):\n",
    "  image = load_img(path)\n",
    "  # Retuns 1024 dimension array/ vector with predicted values \n",
    "  img_embedding = model_embedding.predict(image,steps=1)\n",
    "  # Removes dimensions of size 1 from the shape of a tensor.\n",
    "  img_embedding = tf.squeeze(img_embedding, axis=None, name=None)\n",
    "  # Computes the mean of elements across dimensions of a tensor. [ Normalize ]\n",
    "  img_embedding = tf.reduce_mean(img_embedding, axis=(0,1), keepdims=False, name=None).numpy()\n",
    "  # Converting to List\n",
    "  img_embedding = img_embedding.tolist()\n",
    "  return img_embedding\n",
    "\n",
    "InceptionV3_embeddings = []\n",
    "dir = r'/content/data/women_boots'\n",
    "for filename in os. listdir(dir):\n",
    "  if filename.endswith(\".jpg\"):\n",
    "    InceptionV3_embeddings.append(get_embeddings(os.path.join(dir,filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-XcREVGp1Uc",
    "outputId": "a3bf826c-7ae2-45a6-a237-eb3332538f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5229\n",
      "5229\n",
      "5229\n",
      "5229\n",
      "5229\n",
      "time: 2.36 ms (started: 2021-04-08 08:53:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "print(len(DenseNet121_embeddings))\n",
    "print(len(InceptionV3_embeddings))\n",
    "print(len(MobileNet_embeddings))\n",
    "print(len(ResNet101_embeddings))\n",
    "print(len(ResNet50_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "InLGyfGit1Sy",
    "outputId": "640a5b65-6209-4077-afc3-83b098f28ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.11 s (started: 2021-04-08 09:11:04 +00:00)\n"
     ]
    }
   ],
   "source": [
    "DenseNet121_embeddings_ =numpy.array([numpy.array(xi) for xi in DenseNet121_embeddings])\n",
    "InceptionV3_embeddings_ =numpy.array([numpy.array(xi) for xi in InceptionV3_embeddings])\n",
    "MobileNet_embeddings_ =numpy.array([numpy.array(xi) for xi in MobileNet_embeddings])\n",
    "ResNet101_embeddings_ =numpy.array([numpy.array(xi) for xi in ResNet101_embeddings])\n",
    "ResNet50_embeddings_ =numpy.array([numpy.array(xi) for xi in ResNet50_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1z59J7rjvco6",
    "outputId": "4155f7a6-ca0d-4ac2-fae5-a9c7777ae929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1 ms (started: 2021-04-08 09:08:53 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIsuAnnbtX_O",
    "outputId": "34115f27-4240-4a3b-d752-e2044d04cb91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 76.5 ms (started: 2021-04-08 09:14:27 +00:00)\n"
     ]
    }
   ],
   "source": [
    "ResNet50_embeddings_sparsity = []\n",
    "for i in range(len(ResNet50_embeddings_)):\n",
    "  non_zero = np.count_nonzero(ResNet50_embeddings_[i])\n",
    "  total_val = np.product(ResNet50_embeddings_[i].shape)\n",
    "  sparsity = (total_val - non_zero) / total_val\n",
    "  ResNet50_embeddings_sparsity.append(sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HqBMNTsfvQVw",
    "outputId": "e13ece9a-6ad9-46bd-f0b7-cf90d30e6629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28845375923336203\n",
      "time: 4.11 ms (started: 2021-04-08 09:14:29 +00:00)\n"
     ]
    }
   ],
   "source": [
    "ResNet50_embeddings_sparsity  = Average(ResNet50_embeddings_sparsity)\n",
    "print(ResNet50_embeddings_sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "id": "oBtEv02kwJqk",
    "outputId": "be4a9bd0-18a4-4562-8314-3b19548e2639"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Application</th>\n",
       "      <th>Average Sparsity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DenseNet121</td>\n",
       "      <td>0.000173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>InceptionV3</td>\n",
       "      <td>0.000173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MobileNet</td>\n",
       "      <td>0.048810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ResNet101</td>\n",
       "      <td>0.118761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ResNet50</td>\n",
       "      <td>0.288454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Application  Average Sparsity\n",
       "0  DenseNet121          0.000173\n",
       "1  InceptionV3          0.000173\n",
       "2    MobileNet          0.048810\n",
       "3    ResNet101          0.118761\n",
       "4     ResNet50          0.288454"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 59.1 ms (started: 2021-04-08 09:19:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Application = [\"DenseNet121\",\"InceptionV3\",\"MobileNet\",\"ResNet101\",\"ResNet50\"]\n",
    "Sparsity = [DenseNet121_embeddings_sparsity,DenseNet121_embeddings_sparsity,MobileNet_embeddings_sparsity,ResNet101_embeddings_sparsity,ResNet50_embeddings_sparsity]\n",
    "\n",
    "App_Spar = pd.DataFrame(list(zip(Application, Sparsity)),columns =['Application', 'Average Sparsity'])\n",
    "\n",
    "App_Spar.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "iB6hZGAPyFra",
    "outputId": "a0fe3ff3-0f98-48ac-9fd6-81c3389ca296"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f59dbfe2dd0>"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 211 ms (started: 2021-04-08 09:20:55 +00:00)\n"
     ]
    }
   ],
   "source": [
    "App_Spar.plot.bar(x = 'Application', y = \"Average Sparsity\", rot = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a6bqS5fySsq"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3_Image_Embedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
